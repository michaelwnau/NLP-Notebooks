{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbsnrDKKVarI"
      },
      "outputs": [],
      "source": [
        "# @title # ðŸŒŠ AutoBitnet\n",
        "\n",
        "# @markdown Train your Bitnet model based on LLama Architecture for free on Colab T4 GPU.\n",
        "\n",
        "# @markdown ðŸ”® Created by [@zainulabideen](https://huggingface.co/abideen)\n",
        "\n",
        "# @markdown Based on [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) paper\n",
        "\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### âœ¨ Model Parameters\n",
        "\n",
        "MODEL_CONFIG = \"NousResearch/Llama-2-7b-hf\" # @param {type:\"string\"}\n",
        "HEADS = 6 # @param {type: \"number\"}\n",
        "DIMENSIONS = 768 # @param {type: \"number\"}\n",
        "LAYERS = 6 # @param {type: \"number\"}\n",
        "INTERMEDIATE_SIZE= 1024 # @param {type: \"number\"}\n",
        "CONTEXT_LENGTH = 256 # @param {type: \"number\"}\n",
        "NEW_MODEL = \"Bitnet-Llama-70M\" # @param {type:\"string\"}\n",
        "HUGGINGFACE_ID = \"abideen\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### ðŸ’¥ Training Parameters\n",
        "\n",
        "HF_TOKEN = \"\" # @param {type:\"string\"}\n",
        "WANDB_TOKEN = \"\" # @param {type:\"string\"}\n",
        "DATASET = \"abideen/Cosmopedia-100k-pretrain\" # @param {type:\"string\"}\n",
        "BATCH_SIZE = 8 # @param {type:\"number\"}\n",
        "LEARNING_RATE = 1.5e-3 # @param {type:\"number\"}\n",
        "EPOCHS = 2 # @param {type:\"number\"}\n",
        "!pip install datasets wandb accelerate\n",
        "from torch import nn\n",
        "from transformers.models.llama.modeling_llama import *\n",
        "from transformers import (AutoTokenizer, AutoConfig, LlamaForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments)\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "import wandb\n",
        "from huggingface_hub import create_repo, HfApi\n",
        "\n",
        "def activation_quant(x):\n",
        "    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n",
        "    y = (x * scale).round().clamp_(-128, 127) / scale\n",
        "    return y\n",
        "def weight_quant(w):\n",
        "    scale = 1.0 / w.abs().mean().clamp_(min=1e-5)\n",
        "    u = (w * scale).round().clamp_(-1, 1) / scale\n",
        "    return u\n",
        "\n",
        "class BitLinear(nn.Linear):\n",
        "    def forward(self, x):\n",
        "        w = self.weight # a weight tensor with shape [d, k]\n",
        "        x = x.to(w.device)\n",
        "        RMSNorm = LlamaRMSNorm(x.shape[-1]).to(w.device)\n",
        "        x_norm = RMSNorm(x)\n",
        "        # A trick for implementing Straightâˆ’Throughâˆ’Estimator (STE) using detach()\n",
        "        x_quant = x_norm + (activation_quant(x_norm) - x_norm).detach()\n",
        "        w_quant = w + (weight_quant(w) - w).detach()\n",
        "        y = F.linear(x_quant, w_quant)\n",
        "        return y\n",
        "\n",
        "def convert_to_bitnet(model, copy_weights):\n",
        "    for name, module in model.named_modules():\n",
        "        # Replace linear layers with BitNet\n",
        "        if isinstance(module, LlamaSdpaAttention) or isinstance(module, LlamaMLP):\n",
        "            for child_name, child_module in module.named_children():\n",
        "                if isinstance(child_module, nn.Linear):\n",
        "                    bitlinear = BitLinear(child_module.in_features, child_module.out_features, child_module.bias is not None).to(device=\"cuda:0\")\n",
        "                    if copy_weights:\n",
        "                        bitlinear.weight = child_module.weight\n",
        "                        if child_module.bias is not None:\n",
        "                            bitlinear.bias = child_module.bias\n",
        "                    setattr(module, child_name, bitlinear)\n",
        "        # Remove redundant input_layernorms\n",
        "        elif isinstance(module, LlamaDecoderLayer):\n",
        "            for child_name, child_module in module.named_children():\n",
        "                if isinstance(child_module, LlamaRMSNorm) and child_name == \"input_layernorm\":\n",
        "                    setattr(module, child_name, nn.Identity().to(device=\"cuda:0\"))\n",
        "\n",
        "\n",
        "wandb.login(key=WANDB_TOKEN)\n",
        "login(token=HF_TOKEN)\n",
        "data = load_dataset(DATASET)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG)\n",
        "\n",
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=False,\n",
        "        max_length=CONTEXT_LENGTH,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    # Combine all tokens\n",
        "    combined = []\n",
        "    for tokenized_doc in outputs['input_ids']:\n",
        "        combined += tokenized_doc + [tokenizer.eos_token_id]\n",
        "    # Chunk\n",
        "    input_batch = []\n",
        "    for i in range(0, len(combined) - CONTEXT_LENGTH, CONTEXT_LENGTH):\n",
        "        input_batch.append(combined[i:i+CONTEXT_LENGTH])\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "tokenized_data = data.map(\n",
        "    tokenize, batched=True, remove_columns=data[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "total_tokens = tokenized_data['train'].num_rows * CONTEXT_LENGTH\n",
        "print(f\"Training on {total_tokens:_} tokens\")\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    MODEL_CONFIG,\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=CONTEXT_LENGTH,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "config.hidden_size = DIMENSIONS\n",
        "config.max_position_embeddings = DIMENSIONS\n",
        "config.num_attention_heads = HEADS\n",
        "config.num_hidden_layers = LAYERS\n",
        "config.num_key_value_heads = HEADS\n",
        "config.intermediate_size = INTERMEDIATE_SIZE\n",
        "\n",
        "### Create the llama model with our custom config. Convert it to bitnet.\n",
        "model = LlamaForCausalLM(config)\n",
        "convert_to_bitnet(model, copy_weights=False)\n",
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "output_path = \"./out\"\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_path,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    logging_steps=100,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    save_steps=0.25,\n",
        "    fp16=True,\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(f\"{output_path}/final_model\")\n",
        "folder = \"./out/final_model\"\n",
        "api = HfApi()\n",
        "create_repo(\n",
        "    repo_id = f\"{HUGGINGFACE_ID}/{NEW_MODEL}\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "# Upload gguf files\n",
        "api.upload_folder(\n",
        "    folder_path=folder,\n",
        "    repo_type=\"model\",\n",
        "    repo_id=f\"{HUGGINGFACE_ID}/{NEW_MODEL}\",\n",
        "    token=HF_TOKEN,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}